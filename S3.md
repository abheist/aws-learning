- Amazon S3 is one of the main building blocks of AWS
- It's advertised as "Infinitely scaling" storage
- Many websites uses Amazon S3 as a backbone
- Many AWS services use Amazon S3 as an integration as well
- We'll have a step by step approach to S3
- Use cases
	- Backup and storage
	- Disaster recovery
	- Archive
	- Hybrid Cloud Storage
	- Application Hosting
	- Media Hosting
	- Data lakes & big data analytics
	- Software Delivery
	- Static website
- Buckets
	- Amazon S3 allows people to store object (**Files**) in Buckets (**directories**)
	- Buckets must have a globally unique name (across all regions all accounts)
	- Buckets are defined at a the region level
	- S3 looks like a global service but buckets are created in a region
	- Naming conventions
		- No Uppercase, No underscore
		- 3-63 characters long
		- Not an IP
		- Must start with lowercase letter or number
		- Must Not start with prefix **xn--**
		- Must not end with suffix **-s3alias**
- Objects
	- Objects (files) have a key
	- The key is the Full path:
		- s3://my-bucket/my_file.txt
		- s3://my-bucket/my_folder/another_folder/my_file.txt
	- The key is composed of prefix + object_name
		- s3://my-bucket/my_folder/another_folder/my_file.txt
	- There's no concept of "Directories" within buckets
		  (although the UI will trick you to think otherwise)
	- Just keys with very long names that contain slashes ("/")
	- Object values are the content of the body
		- Max. Object Size is 5TB (5000 GB)
		- If uploading more than 5GB, must use "multi-part upload"
	- Metadata (list of text key / value pairs - system or user metadata)
	- Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
	- Version ID (if versioning is enabled)

### Hands on
- Go to S3 dashboard
- Click on "Create bucket", a form will open
- General configuration
	- Bucket name
	- AWS Region
		In AWS, you'll see all bucket whether you selected a region or not. But AWS buckets are regional. They are created within a region.
	- Copy settings from existing bucket - optional
- Object ownership
	- ACLs disabled (recommended)
		All objects in this bucket are owned by this account. Access to this bucket and its objects is specified using only policies.
	- ACLs enabled
		Objects this this bucket can be owned by other AWS accounts. Access to this bucket and its objects can be specified using ACLs.
- Block public access settings for this bucket
	- Block all public access (select)
- Bucket versioning
	- Disabled
	- Enabled
- Tags
- Create bucket
- Once the bucket is created, you can select that bucket from the dashboard and get inside
- Under objects, upload objects
- Once uploaded, select and object and check properties, permissions, versions

### Security
- User based
	- IAM Policies - which API calls should be allowed for a specific user from IAM
- Resource Based
	- Bucket Policies - bucket wide rules from the S3 console - allows cross account
	- Object Access Control List (ACL) - finer grain (can be disabled)
	- Bucket Access Control List (ACL) - less common (can be disabled)
- Note: an IAM Principal can access an S3 object if
	- The user IAM permissions allow it or the resource policy allows it
	- and there's no explicit deny
- Encryption: encrypt objects in Amazon S3 using encryption keys

### S3 Bucket Policies
```json
"Version": "2012-10-17",
"Statement": [
	{
		"Sid": "PublicRead",
		"Effect": "Allow",
		"Principal": "*",
		"Action": [
			"s3:GetObject"
		],
		"Resource": [
			"arn:aws:s3:::examplebucket/*"
		]
	}
]
```
- JSON based policies
	- Resources: buckets and objects
	- Effect: Allow / Deny
	- Actions: Set of API to Allow or Deny
	- Principal: The account of user to apply the policy to
- Use S3 bucket for policy to:
	- Grant public access to the bucket
	- Force objects to be encrypted at upload
	- Grant access to another account (Cross Account)
- Example: 
	- Public Access - Use Bucket Policy
	- User Access to S3 - IAM permissions
	- EC2 instance access - Use IAM Role
	- Cross Account Access - Use Bucket Policy
- Bucket settings for Block Public Access![Screenshot 2023-06-30 at 12.49.07 PM](images%201/Screenshot%202023-06-30%20at%2012.49.07%20PM.png)
	- These settings were created to prevent company data leaks
	- If you know your bucket should never be public, leave these on
	- Can be set at the account level

### Static Website Hosting
- S3 can host static websites and have them accessible on the internet
- The website URL will be (depending on the region)
	- http://bucket-name.s3-website-aws-region.amazonaws.com
	- or
	- http://bucket-name.s3-website.aws-region.amazonaws.com
- If you get a 403 Forbidden error, make sure the bucket policy allows public reads!
- Hands on
	- Go to bucket
	- Properties
	- and at the end, you'll find "Static Website Hosting"
	- Enable
	- Hosting type
		- Host a static website
		- Redirect requests for an object
	- Index document
		- index.html
	- Error document
	- Save
	- Upload index.html and other required files on the bucket
	- Once all done, you will find a new URL under properties, static website hosting

### Versioning
- You can version your files in Amazon S3
- It is enabled at the bucket level
- Same key overwrite will change the "version": 1,2,3....
- It is best practice to version your buckets
	- Protect against unintended deletes (ability to restore a version)
	- Easy roll back to previous version
- Notes:
	- Any file that is not versioned prior to enabling versioning will have version "null"
	- Suspending versioning does not delete the previous versions
- Hands on
	- Under bucket properties
	- Bucket versioning
		- Enable
	- Save changes

### Replication
- two types
	- CRR: Cross region replication
	- SRR: Same region replication
- Must enable versioning in source and destination buckets
- Bucket can be in different AWS accounts
- Copying is asynchronous
- Must give proper IAM permissions to S3
-  Use cases:
	- CRR - compliance, lower latency access, replication across accounts
	- SRR - log aggregation, live replication between production and test accounts
- After you enable Replication, only new objects are replicated
- Optionally, you can replicate existing objects using S3 Batch Replication
	- Replicates, existing objects and objects that failed replication
- For delete operations
	- Can replicate delete marker from source to target (optional setting)
	- Deletions with version ID are not replicated (to avoid malicious deletes)
- There is no "Chaining" of replication
	- If bucket 1 has replication into bucket 2, which has replication into bucket 3
	- Then objects created in bucket 1 are not replicated to bucket 3
- Hands on
	- Create a new bucket-1
		- Enable versioning
		- Create
	- Create a new bucket-2
		- Enable versioning
		- Create
	- No go to bucket-1 and under Management tab
	- Go to `Replication Rules`
	- Create new replication rule
		-  Rule name
		- Status
		- Choose a rule scope
			- Limit the scope of this rule using one or more filters
			- Apply to all objects in the bucket (select)
		- Destination bucket
			- Choose a bucket in this account
				- Enter target bucket name
					- `bucket-2`
		-  IAM role
			- select `Create new role` from dropdown
	- Save
- To replicate the delete marker replication, you need to enable it from management, and edit replication rule.

### S3 Storage Classes
- Amazon S3 Standard - General Purpose
	- 99.99% availability
	- Used for frequently accessed data
	- Low latency and high throughput
	- Sustain 2 concurrent facility failures
	- Use cases: Big data analytics, mobile & gaming applications, content distribution...
- Amazon S3 Infrequent Access (IA)
	- Standard-IA
		- For data that is less frequently accessed, but requires rapid access when needed
		- Lower cost than S3 Standard
		- 99.9% availability
		- Use cases: Disaster Recovery, backups
	- One Zone-IA
		- High durability (99.999999999%) in a single AZ, data lost when AZ is destroyed
		- 99.5% availability
		- Use cases: Storing secondary backup copies of on-premise data, or data you can recreate
- Amazon S3 Glacier
		Low cost storage meant for archiving/backup
		Pricing: price for storage + object retrieval cost
	- **Instant Retrieval**
		- Millisecond retrieval, great for data accessed once a quarter
		- Minimum storage duration for 90 days
	- **Flexible Retrieval**
		- Expedited (1 to 5 minutes)
		- Standard (3 to 5 hours)
		- Bulk (5 to 12 hours) - free
		- Minimum storage duration of 90 days
	- **Deep Archive**
		- for long term storage
		- Standard (12 hours)
		- Bulk (48 hours)
		- Minimum storage duration of 180 days
- Amazon S3 Intelligent Tiering
	- Small monthly monitoring and auto-tiering fee
	- Moves objects automatically between Access Tiers based on usage
	- There are no retrieval charges in S3 Intelligent Tiering
	- Tiers
		- Frequent Access Tier (automatic): default tier
		- Infrequent Access Tier (automatic): objects not accessed for 30 days
		- Archive Instant  Access Tier (automatic): object not accessed for 90 days
		- Archive Access Tier (optional): configurable from 90 days to 700+ days
		- Deep Archive Access tier (optional): config. 180 days to 700+ days
- Comparison![Screenshot 2023-06-30 at 1.59.22 PM](images%201/Screenshot%202023-06-30%20at%201.59.22%20PM.png)
- Pricing![Screenshot 2023-06-30 at 1.59.46 PM](images%201/Screenshot%202023-06-30%20at%201.59.46%20PM.png)
- Can move between classes manually or using S3 Lifecycle configurations

### S3 Durability and Availability
- Durability
	- High durability (99.99999999999% ,  11 9s) of objects across multiple AZ
	- if you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object every 10,000 years
	- Same for all storage classes
- Availability
	- Measures how readily available a service is
	- Varies depending on storage class
	- Example: S3 standard has 99.99% availability = not available 53 minutes a year

### Amazon S3 Analytics - Storage Class Analysis
- Help you decide when to transition objects to the right storage class
- Recommendations for Standard and Standard IA
	- Does Not work for One-Zone IA or Glacier
- Report is updated daily
- 24 to 48 hours to start seeing data analysis
- Good first step to put together LifeCycle Rules (or improve them)!

### Lifecycle Rules
- Moving between storage classes
- You can transition objects between storage classes![Screenshot 2023-07-01 at 11.38.53 AM](images%201/Screenshot%202023-07-01%20at%2011.38.53%20AM.png)
- For infrequent accessed object, move them to Standard IA
- For archive objects that you don't need fast access to, move them to Glacier or Glacier Deep Archive
- Moving objects can be automated using a Lifecycle Rules
- LifeCycle Rules
	- Transition Actions
		- Configure objects to transition to another storage class
		- Move objects to Standard IA class 60 days after creation
		- Move to Glacier for archiving after 6 months
	- Expiration actions
		- Configure objects to expire (delete) after some time
		- Access log files can be set to delete after a 365 days
		- Can be used to delete old versions of files (if versioning is enabled)
		- Can be used to delete incomplete Multi-Part uploads
	- Rules can be created for certain prefixes (example: s3://mybucket/mp3/*)
	- Rules can be created for certain objects Tags (example: Department: Finance)
- Hands on
	- Under S3 bucket Management Tab, create Lifecycle rule
	- A new form will open
	- Lifecycle rule configurations
		- Name
		- Choose rule scope
			- Limit the scope of this rule using one or more filters
			- Apply to all objects in the bucket
		- Lifecycle rule actions
			- Move current versions of objects between storage classes
			- Move non-current versions of objects between storage classes
			- Expire current versions of objects
			- Permanently delete non-current versions of objects
			- Delete expired object delete markers or incomplete multipart uploads
				  These actions are not supported when filtering by object tags or size
		- Transition days settings
	- Save
	- Now this rule will act in the background as defined

### S3 Event Notifications
- Events
	- S3.ObjectCreated
	- S3:ObjectRemoved
	- S3:ObjectRestore
	- S3:Replication
- Object name filtering possible (\*.jpg)
- Use case: generate thumbnails of images uploaded to S3
- Can create as many "S3 events" as desired
- S3 event notifications typically deliver events in seconds but can sometimes take minute or longer![Screenshot 2023-07-01 at 12.04.39 PM](images%201/Screenshot%202023-07-01%20at%2012.04.39%20PM.png)
- IAM Permissions for Event notifications to work
	- Need to create Access Policy on target service![Screenshot 2023-07-01 at 12.06.24 PM](images%201/Screenshot%202023-07-01%20at%2012.06.24%20PM.png)
- S3 Event Notifications with Amazon [EventBridge](Monitoring%20&%20Logs/EventBridge.md)![Screenshot 2023-07-01 at 12.09.52 PM](images%201/Screenshot%202023-07-01%20at%2012.09.52%20PM.png)
	- Advanced filtering options with JSON rules (metadata, object size, name...)
	- Multiple Destination - ex Step Functions, Kinesis Streams / Firehose...
	- EventBridge Capabilities - Archive, Replay Events, Reliable delivery 
- Hands on
	- In bucket, under properties, create Event Notification, a new form will open
	- General configurations
		- Event Name
		- Prefix - optional
		- Suffix - optional
	- Event types
		- Object Creation
			- select - All object create events (s3:ObjectCreated:\*)
	- Destination
		- [Lambda](Lambda/Lambda.md) function
		- [SNS](Messaging%20&%20Queues/SNS.md) topic
		- [SQS](Messaging%20&%20Queues/SQS.md) queue (select)
			- Create SQS Queue
			- Change the access policy, so S3 can send notifications to it.
	- Save


### S3 Baseline Performance
- Amazon S3 automatically scales to high request rates, latency 100-200 ms
- Your application can achieve 3500 PUT/COPY/POST/DELETE or 5500 GET/HEAD requests per second per prefix in a bucket
- There is no limit to the number of prefixes in a bucket
- Performance
	- Multi-part upload:
		- recommended for files > 100 MB, must use for files > 5GB
		- Can help parallelize uploads (speed up transfers)![Screenshot 2023-07-01 at 12.31.23 PM](images%201/Screenshot%202023-07-01%20at%2012.31.23%20PM.png)
	- S3 Transfer Acceleration
		- Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region
		- Compatible with multi-part upload![Screenshot 2023-07-01 at 12.32.31 PM](images%201/Screenshot%202023-07-01%20at%2012.32.31%20PM.png)
	- S3 Byte-Range fetches
		- Parallelize GETs by requesting specific byte ranges
		- Better resilience in case of failures
		- Can be used to speed up downloads![Screenshot 2023-07-01 at 12.35.16 PM](images%201/Screenshot%202023-07-01%20at%2012.35.16%20PM.png)
		- Can be used to retrieve only partial data (for example the head of a file)![Screenshot 2023-07-01 at 12.35.25 PM](images%201/Screenshot%202023-07-01%20at%2012.35.25%20PM.png)

### S3 Select & Glacier Select
- Retrieve less data using SQL by performing server-side filtering
- Can filter by rows & columns (simple SQL statements)
- Less network transfer, less CPU cost client-side![Screenshot 2023-07-01 at 12.37.37 PM](images%201/Screenshot%202023-07-01%20at%2012.37.37%20PM.png)

### S3 User-Defined Object Metadata & S3 Object Tags
- S3 User-Defined Object Metadata
	- When uploading an object, you can also assign metadata
	- Name-Value (key-value) pairs
	- Use-defined metadata names must begin with `"z-amz-meta-"`
	- Amazon S3 stores user-defined metadata keys in lowercase
	- Metadata can be retrieved while retrieving the object
- S3 Object Tags
	- Key-value pairs for objects in Amazon S3
	- Useful for fine-grained permissions (only access specific objects with specific tags)
	- Useful for analytics purposes (using S3 Analytics to group by tags)
- You cannot search the object metadata or object tags
- Instead, you must use an external DB as a search index such as DynamoDB

## S3 Security

### S3 Object Encryption
- You can encrypt objects in S3 buckets using one of 4 methods
- Server-Side Encryption (SSE)
	- Server Side Encryption with Amazon S3-Managed Keys (SSE-S3) - Enabled by default
		- Encrypt S3 objects using keys handled, managed, and owned by AWS
		- Object is encrypted server side
		- Encryption type is AES-256
		- Must set header `"z-amz-server-side-encryption": "AES256"`
		- Enabled by default for new buckets & new objects![Screenshot 2023-07-01 at 2.01.52 PM](images%201/Screenshot%202023-07-01%20at%202.01.52%20PM.png)
	- Server Side Encryption with KMS keys stored in AWS KMS (SSE_KMS)
		- Leverage AWS Key Management Service (AWS KMS) to manage encryption keys
		- Encryption using keys handled and managed by AWS KMS (Key Management Service)
		- KMS advantages: user control + audit key usage using CloudTrail
		- Object is encrypted server side
		- Must set header `"x-amz-server-side-encryption": "aws:kms`![Screenshot 2023-07-01 at 1.39.04 PM](images%201/Screenshot%202023-07-01%20at%201.39.04%20PM.png)
		- SSE-KMS Limitations
			- If you use SSE_KMS, you may be impacted by the KMS limits
			- When you upload, it call the GenerateDataKey KMS API
			- When you download, it calls the Decrypt KMS API
			- Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region)
			- You can request a quota increase using the Service Quotas Console
	- Server Side Encryption with Customer-Provided Keys (SSE-C)
		- When you want to manage your own encryption keys
		- Server-side encryption using keys fully managed by the customer outside of AWS
		- Amazon S3 does not store the encryption key you provide
		- HTTPS must be used
		- Encryption key must provided in HTTP headers, for every HTTP request made![Screenshot 2023-07-01 at 2.08.33 PM](images%201/Screenshot%202023-07-01%20at%202.08.33%20PM.png)
	- Client-Side Encryption
		- Use client libraries such as Amazon S3 Client-Side encryption library
		- Client must encrypt data themselves before sending to Amazon S3
		- Clients must decrypt data themselves when retrieving from Amazon S3
		- Customer fully manages the keys and encryption cycle![Screenshot 2023-07-01 at 2.10.41 PM](images%201/Screenshot%202023-07-01%20at%202.10.41%20PM.png)
	- Encryption in transit (SSL/TLS)
		- Amazon S3 exposes two endpoints
			- HTTP Endpoint - non encrypted
			- HTTPS Endpoint - encryption in flight
		- HTTPS is recommended
		- HTTPS is mandatory for SSE-C
		- Most clients would use the HTTPS endpoint by default
		- Force Encryption in Transit
			- It can be forced by adding `aws:SecureTransport` in the policy
			```json
{
	"Version": "2012-10-17",
	"Statement": {
		"Effect": "Deny",
		"Principal": "*",
		"Action": "s3:GetObject",
		"Resource": "arn:aws:s3:::my-bucket/*",
		"Condition": {
			"Bool" : {
				"aws:SecureTransport": "false"
			}
		}
	}
}
			```
- It's important to understand which ones are for which situation for the exam

### Default Encryption and bucket policies
- SSE-S3 encryption is automatically applied to new objects stored in S3 bucket
- Optionally, you can "force encryption" using a bucket policy and refuse any API call to PUT an S3 object without encryption headers (SSE-KMS or SSE-C)![Screenshot 2023-07-02 at 11.57.07 AM](images%201/Screenshot%202023-07-02%20at%2011.57.07%20AM.png)
- This can be changed under properties
> [!info] Bucket policies are evaluated before "Default Encryption"

### S3 CORS
- Cross-Origin Resource Sharing (CORS)
- Origin = scheme (protocol) + host (domain) + port
	- example: https://www.example.com (implied port is 433 for HTTPS, 80 for HTTP)
- Web browser based mechanism to allow requests to other origins while visiting the main origin
- Same origin: http://example.com/app1 & http://example.com/app2
- Different Origins: http://www,example.com & http://other.example.com
- The requests won't be fulfilled unless the other origin allows for the requests, using CORS Headers (example: Access-Control-Allow-Origin)
- If a client makes a cross-origin request on our S3 bucket, we need to enable the correct CORS headers
- It's a popular exam question
- You can allow for a specific origin or for * (all origins).
- Can be set under properties, CORS settings

### MFA Delete
- MFA (Multi-Factor Authentication) - force users to generate a code on a device (usually a mobile phone or hardware) before doing important operations on S3
- MFA will be required to:
	- Permanently delete an object version
	- Suspend versioning on the bucket
- MFA won't be required to:
	- Enable versioning
	- List deleted versions
- To use MFA delete, versioning must be enabled on the bucket
- Only the bucket owner (root account) can enable/disable MFA Delete
> [!info] MFA delete can only be enabled by AWS CLI, AWS SDK, or the Amazon S3 Rest API
- Hands on
	- Configure AWS cli
	- now run the following command to enable MFA for S3 delete
	```sh
aws s3api put-bucket-versioning --bucket mfa-demo-abheist --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa "arn-of-mda-device mfa-code" --profile root-mfa-delete-demo
	```
	- Once done, you can find this setting is changed under properties under bucket versioning

### S3 Access Logs
- For audit purpose, you may want to log all access to S3 buckets
- Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
- That data can be analyzed using data analysis tools
- The target logging bucket must be in the same AWS region
- The log format is at:
	- https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html
- Warning
	- Do not set your logging bucket to be the monitoring bucket
	- It will create a logging loop, and your bucket will grow exponentially.
- Hands on
	- This can be enabled from properties under `Server Access Logging`

### S3 Pre-Signed URLS
- Generate pre-signed URLs using the S3 console, AWS CLI or SDK
- URL Expiration
	- S3 Console - 1 min up to 720 mins (12 hours)
	- AWS CLI - configure expiration with --expires-in parameter in seconds (default 3600 secs, max 604800 secs ~168 hours)
- Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET / PUT
- Example:
	- Allow only logged-in users to download a premium video from. your S3 bucket
	- Allow an ever-changing list of users to download files by generating URLs dynamically
	- Allow temporarily a user to upload a file to a precise location in your S3 bucket
- Hands on
	- One option is to do it from CLI
	- Other option is from Action button
	- Open any file and click on Action button
	- and select `Share with a presigned URL`
	- Add mins/hrs 
	- Create
	- Copy the URL and share

### S3 Access Points![Screenshot 2023-07-02 at 1.23.47 PM](images%201/Screenshot%202023-07-02%20at%201.23.47%20PM.png)
- Access Points simplify security management for S3 buckets
- Each access point has
	- its own DNS name (internet origin or VPC origin)
	- an access point policy (similar to bucket policy) - manage security at scale
- VPC origin
	- We can define the access point to be accessible only from within the VPC
	- You must create a VPC endpoint to access the Access Point (Gateway or Interface Endpoint)
	- The VPC Endpoint Policy must allow access to the target bucket and Access Point

### S3 Object Lambda![Screenshot 2023-07-02 at 1.30.28 PM](images%201/Screenshot%202023-07-02%20at%201.30.28%20PM.png)
- Use AWS Lambda Function to change the object before it is retrieved by the called application
- Only one S3 bucket is needed, on top which we create S3 Access Point and S3 Object Lambda Access Points.
- Use cases:
	- Redacting personally identifiable information for analytics or non-production environments.
	- Converting across data formats, such as converting XML to JSON
	- Resizing and watermarking images on the fly using caller-specific details, such as the user who requested the object.
